{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3e8b9f-ccf2-45cd-8218-e19dbdb8de6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Imbalanced clustering metrics for benchmarking, analysis, and comparison studies** \n",
    "\n",
    "`imbalanced-clustering` is a python library that has repurposed popular clustering indices such as the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) to account for class imbalance. Although the original intended use of these metrics was to compare clustering results from different techniques, they have recently been applied frequently in cases where ground-truth labels exist and the result of a clustering technique is compared to  these values. An example of this is in single-cell sequencing benchmarking studies, where ground-truth celltype labels are compared to clustering results after a meaningful transformation of the latent space, such as the integration of two or more datasets. \n",
    "\n",
    "Although these metrics are meaningful in these settings, they often overemphasize the importance of larger classes in the data. In single-cell datasets, it's common that some celltypes exist in greater proportions than others in a given tissue sample, and they will thus be overrepresented in the sequencing data (e.g. alpha and beta cells from pancreatic islet samples). Because of this overrepresentation, the larger classes will have a greater influence on the results of these common clustering metrics. This may hide important information about smaller classes/celltypes, and potential erasure of their heterogeneity in a clustering setting. This problem is akin to the imbalanced learning problem in classical machine learning literature, where metrics such as accuracy and precision/recall can fail to capture the effects of a classifier on smaller/minority classes. \n",
    "\n",
    "Given this limitation, the `imbalanced-clustering` python library balances popular clustering metrics, such that the information from each of the classes from the ground-truth data, regardless of their representation in the data, will be weighted equally. \n",
    "\n",
    "The following demo notebook is divided up into two parts:\n",
    "\n",
    "**A)**: First we begin by examining how this weighing is done to account for ground-truth class imbalance by specifically considering the Adjusted Rand Index (ARI), as the other metrics undergo the exact same changes.\n",
    "\n",
    "**B)**: The feasibility of these metrics, as well as their concordance with the base imbalanced metrics is determined. To begin, sanity checks and control experiments are done to determine if the same results hold for both the base and modified balanced metrics. Then, different class/clustering scenarios are explored that emphasize the utility of the balanced metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d930a-16c4-4adf-b692-d93121cc7a09",
   "metadata": {},
   "source": [
    "### Part A) Analysis of reweighting of the Adjusted Rand Index  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bf532-c754-4b86-bbb5-46eba0bdb47e",
   "metadata": {},
   "source": [
    "### Part B) Computational analysis of balanced metrics \n",
    "\n",
    "In this part of the demo, we'll look at some of the properties of these balanced metrics, perform some sanity checks to ensure they are working as intended, and demonstrate important use cases and results. \n",
    "\n",
    "For **Part B-1)**, we'll start by looking at some baselines to ensure that the balanced metrics return similar results as the baseline metrics in edge cases and randomized data. There are two properties we want to ensure that exist, namely the expectation - where a random clustering results in a similar value for both, and normalization, where both the balanced and baseline metrics are bounded in the same way. We've ensured that these properties hold theoretically, and we'll demonstrate that they do computationally. \n",
    "\n",
    "Let's begin by loading the appropriate functions and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74aad355-2725-4855-b1e3-5058a0ffc283",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adjusted_rand_score, adjusted_mutual_info_score, \\\n\u001b[1;32m      5\u001b[0m     homogeneity_score, completeness_score, v_measure_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, \\\n",
    "    homogeneity_score, completeness_score, v_measure_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from imbalanced_clustering import balanced_adjusted_rand_index, \\\n",
    "    balanced_adjusted_mutual_info, balanced_completeness, \\\n",
    "    balanced_homogeneity, balanced_v_measure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0065da-0188-4af7-b801-b0a27d132a04",
   "metadata": {},
   "source": [
    "We can begin testing random clustering by sampling data from a uniform distribution for 3 classes, and then performing k-means clustering on them. Let's see how the metrics behave when the classes are balanced (same number in each) in this respect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587af53c-352c-4936-881c-9a2258da48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility \n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample three classes from a uniform distribution\n",
    "a = np.random.uniform(0, 100, (500, 2))\n",
    "b = np.random.uniform(0, 100, (500, 2))\n",
    "c = np.random.uniform(0, 100, (500, 2))\n",
    "\n",
    "# Plot the given results for each class \n",
    "cluster_df = pd.DataFrame({\n",
    "    \"x\" : np.concatenate((c_1[:, 0], c_2[:, 0], c_3[:, 0])),\n",
    "    \"y\" : np.concatenate((c_1[:, 1], c_2[:, 1], c_3[:, 1])),\n",
    "    \"cluster\": np.concatenate(\n",
    "        (\n",
    "            np.repeat(\"A\", len(c_1)),\n",
    "            np.repeat(\"B\", len(c_2)),\n",
    "            np.repeat(\"C\", len(c_3))\n",
    "        )\n",
    "    )\n",
    "})\n",
    "sns.scatterplot(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    hue = \"cluster\",\n",
    "    data = cluster_df\n",
    ")\n",
    "\n",
    "# Perform k-means clustering and plot the results  \n",
    "cluster_arr = np.array(cluster_df.iloc[:, 0:2])\n",
    "kmeans_res = cluster.KMeans(n_clusters = 2).fit_predict(X = cluster_arr)\n",
    "cluster_df[\"kmeans\"] = kmeans_res\n",
    "sns.scatterplot(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    hue = \"kmeans\",\n",
    "    data = cluster_df\n",
    ")\n",
    "\n",
    "# Determine the values for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imba_clus",
   "language": "python",
   "name": "imba_clus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
